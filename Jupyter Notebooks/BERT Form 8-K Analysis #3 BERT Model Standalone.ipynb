{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\bert\\optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3063: DtypeWarning: Columns (35,36,37,38,39,41,42,43,47,48,49,51,60,61) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from datetime import datetime\n",
    "from tensorflow import keras\n",
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "print(tf.__version__)\n",
    "df = pd.read_csv('F:\\Files from Linux/cleaneddataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell for a function for oversampling\n",
    "\n",
    "def oversample(X,y):\n",
    "    # Get number of rows with imbalanced class\n",
    "    target = y.sum().idxmax()\n",
    "    n = y[target].sum()\n",
    "    # identify imbalanced targets\n",
    "    imbalanced = y.drop(target,axis=1)\n",
    "    #For each target, create a dataframe of randomly sampled rows, append to list\n",
    "    append_list =  [y.loc[y[col]==1].sample(n=n-y[col].sum(),replace=True,random_state=20) for col in imbalanced.columns]\n",
    "    append_list.append(y)\n",
    "    y = pd.concat(append_list,axis=0)\n",
    "    # match y indexes on other inputs\n",
    "    X = X.loc[y.index]\n",
    "    assert (y.index.all() == X.index.all())\n",
    "    return X, y\n",
    "\n",
    "df = df.rename(columns = {\"cleantext\": \"filtered_text\"})\n",
    "df = df.sort_values(by='release_date', ascending=True, axis=0)\n",
    "testNum = int(len(df) * -.1)\n",
    "X_train = df['filtered_text'][:testNum].dropna()\n",
    "y_train = pd.get_dummies(columns=['target'],data=df['target'])[:testNum].dropna().iloc[:, :]\n",
    "test = df.loc[list(set(list(df.index)) - set(list(X_train.index)))]\n",
    "X_test = test['filtered_text'].dropna()\n",
    "y_test = test['target'].dropna()\n",
    "\n",
    "X_train, y_train = oversample(X_train, y_train)\n",
    "\n",
    "# Recreate the target variable\n",
    "y_train[\"target\"] = np.nan\n",
    "\n",
    "for i, y in y_train.iterrows():\n",
    "    if str(type(y_train.loc[i])) == \"<class 'pandas.core.frame.DataFrame'>\": # If an index only has one observation, it draws up an error if we try to use the indexer agaon\n",
    "        # They're usually classed as a series while the ones with many observations are considered a df. This is a way to get\n",
    "        # rid of them\n",
    "        if y_train.loc[i].iloc[0, 0] == 1:\n",
    "            y_train.loc[i, \"target\"] = \"Negative\"\n",
    "        elif y_train.loc[i].iloc[0, 1] == 1:\n",
    "            y_train.loc[i, \"target\"] = \"Neutral\"\n",
    "        else:\n",
    "            y_train.loc[i, \"target\"] = \"Positive\"\n",
    "    else: # If they only have one observation, we settle it here instead\n",
    "        if y_train.loc[i][0] == 1:\n",
    "              y_train.loc[i, \"target\"] = \"Negative\"\n",
    "        elif y_train.loc[i][1] == 1:\n",
    "            y_train.loc[i, \"target\"] = \"Neutral\"\n",
    "        else:\n",
    "            y_train.loc[i, \"target\"] = \"Positive\"\n",
    "            \n",
    "X_train2 = X_train.reset_index(drop = True)\n",
    "y_train2 = y_train['target'].reset_index(drop = True)\n",
    "\n",
    "data = pd.concat([X_train2, y_train2], axis = 1)\n",
    "data.rename(columns = {\"filtered_text\":\"doc\"}, inplace = True)\n",
    "\n",
    "\n",
    "\n",
    "X_test = X_test.reset_index(drop = True)\n",
    "y_test = y_test.reset_index(drop = True)\n",
    "\n",
    "\n",
    "#Added a random sample to make sure data is randomized going into BERT to help with overfitting\n",
    "#as there are many negatives added at the front of the data in a row due to oversampling\n",
    "train = data.sample(frac=1)\n",
    "\n",
    "\n",
    "\n",
    "test = pd.concat([X_test, y_test], axis = 1) \n",
    "test.rename(columns = {\"filtered_text\":\"doc\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10071\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\bert\\tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\bert\\tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_COLUMN = 'doc'\n",
    "LABEL_COLUMN = 'target'\n",
    "# label_list is the list of labels\n",
    "label_list = ['Positive', 'Neutral', 'Negative']\n",
    "\n",
    "\n",
    "train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "\n",
    "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "\n",
    "#BERT_MODEL_HUB = \"https://tfhub.dev/google/albert_base/3\"\n",
    "\n",
    "def create_tokenizer_from_hub_module():\n",
    " # \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    with tf.Graph().as_default():\n",
    "        bert_module = hub.Module(BERT_MODEL_HUB)\n",
    "        tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "        with tf.Session() as sess:\n",
    "            vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],tokenization_info[\"do_lower_case\"]])\n",
    "      \n",
    "    return bert.tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "tokenizer = create_tokenizer_from_hub_module()\n",
    "\n",
    "# We'll set sequences to be at most 128 tokens long.\n",
    "MAX_SEQ_LENGTH = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\bert\\run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\bert\\run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 10071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 10071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] january 25 , 2019 , the board approved and adopted effective immediately an amendment ( the “ amendment ” ) to the by ##law ##s of the company ( the “ by ##law ##s ” ) to remove the restriction in article ii , section 14 ( b ) ( xii ) that any shareholder nominee ( as defined in article ii , section 14 of the by ##law ##s ) who is included in the company ’ s proxy materials for a particular meeting of shareholders but does not receive at least twenty - five percent ( 25 % ) of the votes cast in favor of the shareholder nominee ’ s election , shall be ineligible to be a shareholder nominee pursuant to sub ##section ( b ) of section 14 of article ii of the by ##law ##s for the next two annual meetings of shareholders following the meeting for which the shareholder nominee has been included in the company ’ s proxy materials . the fore ##going description of the amendment does not pu ##rp ##ort to be complete and is qualified in its entirety by reference to the full text of the by ##law ##s , attached here ##to as exhibit 3 . 1 and incorporated here ##in by reference . signature ##pur ##su ##ant to the requirements of the securities exchange act of 1934 , the regis ##tra ##nt has duly caused this report to be signed on its behalf by the under ##si ##gned here ##unt ##o [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] january 25 , 2019 , the board approved and adopted effective immediately an amendment ( the “ amendment ” ) to the by ##law ##s of the company ( the “ by ##law ##s ” ) to remove the restriction in article ii , section 14 ( b ) ( xii ) that any shareholder nominee ( as defined in article ii , section 14 of the by ##law ##s ) who is included in the company ’ s proxy materials for a particular meeting of shareholders but does not receive at least twenty - five percent ( 25 % ) of the votes cast in favor of the shareholder nominee ’ s election , shall be ineligible to be a shareholder nominee pursuant to sub ##section ( b ) of section 14 of article ii of the by ##law ##s for the next two annual meetings of shareholders following the meeting for which the shareholder nominee has been included in the company ’ s proxy materials . the fore ##going description of the amendment does not pu ##rp ##ort to be complete and is qualified in its entirety by reference to the full text of the by ##law ##s , attached here ##to as exhibit 3 . 1 and incorporated here ##in by reference . signature ##pur ##su ##ant to the requirements of the securities exchange act of 1934 , the regis ##tra ##nt has duly caused this report to be signed on its behalf by the under ##si ##gned here ##unt ##o [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2254 2423 1010 10476 1010 1996 2604 4844 1998 4233 4621 3202 2019 7450 1006 1996 1523 7450 1524 1007 2000 1996 2011 14919 2015 1997 1996 2194 1006 1996 1523 2011 14919 2015 1524 1007 2000 6366 1996 16840 1999 3720 2462 1010 2930 2403 1006 1038 1007 1006 14371 1007 2008 2151 18668 9773 1006 2004 4225 1999 3720 2462 1010 2930 2403 1997 1996 2011 14919 2015 1007 2040 2003 2443 1999 1996 2194 1521 1055 24540 4475 2005 1037 3327 3116 1997 15337 2021 2515 2025 4374 2012 2560 3174 1011 2274 3867 1006 2423 1003 1007 1997 1996 4494 3459 1999 5684 1997 1996 18668 9773 1521 1055 2602 1010 4618 2022 22023 2000 2022 1037 18668 9773 27081 2000 4942 29015 1006 1038 1007 1997 2930 2403 1997 3720 2462 1997 1996 2011 14919 2015 2005 1996 2279 2048 3296 6295 1997 15337 2206 1996 3116 2005 2029 1996 18668 9773 2038 2042 2443 1999 1996 2194 1521 1055 24540 4475 1012 1996 18921 26966 6412 1997 1996 7450 2515 2025 16405 14536 11589 2000 2022 3143 1998 2003 4591 1999 2049 15700 2011 4431 2000 1996 2440 3793 1997 1996 2011 14919 2015 1010 4987 2182 3406 2004 8327 1017 1012 1015 1998 5100 2182 2378 2011 4431 1012 8085 5311 6342 4630 2000 1996 5918 1997 1996 12012 3863 2552 1997 4579 1010 1996 20588 6494 3372 2038 25073 3303 2023 3189 2000 2022 2772 2006 2049 6852 2011 1996 2104 5332 19225 2182 16671 2080 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2254 2423 1010 10476 1010 1996 2604 4844 1998 4233 4621 3202 2019 7450 1006 1996 1523 7450 1524 1007 2000 1996 2011 14919 2015 1997 1996 2194 1006 1996 1523 2011 14919 2015 1524 1007 2000 6366 1996 16840 1999 3720 2462 1010 2930 2403 1006 1038 1007 1006 14371 1007 2008 2151 18668 9773 1006 2004 4225 1999 3720 2462 1010 2930 2403 1997 1996 2011 14919 2015 1007 2040 2003 2443 1999 1996 2194 1521 1055 24540 4475 2005 1037 3327 3116 1997 15337 2021 2515 2025 4374 2012 2560 3174 1011 2274 3867 1006 2423 1003 1007 1997 1996 4494 3459 1999 5684 1997 1996 18668 9773 1521 1055 2602 1010 4618 2022 22023 2000 2022 1037 18668 9773 27081 2000 4942 29015 1006 1038 1007 1997 2930 2403 1997 3720 2462 1997 1996 2011 14919 2015 2005 1996 2279 2048 3296 6295 1997 15337 2206 1996 3116 2005 2029 1996 18668 9773 2038 2042 2443 1999 1996 2194 1521 1055 24540 4475 1012 1996 18921 26966 6412 1997 1996 7450 2515 2025 16405 14536 11589 2000 2022 3143 1998 2003 4591 1999 2049 15700 2011 4431 2000 1996 2440 3793 1997 1996 2011 14919 2015 1010 4987 2182 3406 2004 8327 1017 1012 1015 1998 5100 2182 2378 2011 4431 1012 8085 5311 6342 4630 2000 1996 5918 1997 1996 12012 3863 2552 1997 4579 1010 1996 20588 6494 3372 2038 25073 3303 2023 3189 2000 2022 2772 2006 2049 6852 2011 1996 2104 5332 19225 2182 16671 2080 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: Neutral (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: Neutral (id = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] 4 , 2019 , new ##mont mining corporation ( the company or new ##mont ) announced that shareholders of gold ##corp inc . ( gold ##corp ) approved an arrangement to combine with new ##mont ( the proposed new ##mont gold ##corp transaction ) at a special meeting of shareholders held on april 4 , 2019 . gold ##corp announced that shareholders voted overwhelmingly in favor of combining with new ##mont , with more than 97 percent of votes cast in favor of the proposed new ##mont gold ##corp transaction . a copy of new ##mont ##s press release is attached as exhibit 99 . 1 and is incorporated into this item 8 . 01 by reference . caution ##ary statement regarding forward - looking statements this current report on form 8 - k contains forward - looking statements within the meaning of section 27 ##a of the securities act of 1933 , as amended , and section 21 ##e of the securities exchange act of 1934 , as amended , which are intended to be covered by the safe harbor created by such sections and other applicable laws and forward - looking information within the meaning of applicable canadian securities laws . where a forward - looking statement expresses or implies an expectation or belief as to future events or results , such expectation or belief is expressed in good faith and believed to have a reasonable basis . however , such statements are subject to risks , uncertain ##ties and other factors [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] 4 , 2019 , new ##mont mining corporation ( the company or new ##mont ) announced that shareholders of gold ##corp inc . ( gold ##corp ) approved an arrangement to combine with new ##mont ( the proposed new ##mont gold ##corp transaction ) at a special meeting of shareholders held on april 4 , 2019 . gold ##corp announced that shareholders voted overwhelmingly in favor of combining with new ##mont , with more than 97 percent of votes cast in favor of the proposed new ##mont gold ##corp transaction . a copy of new ##mont ##s press release is attached as exhibit 99 . 1 and is incorporated into this item 8 . 01 by reference . caution ##ary statement regarding forward - looking statements this current report on form 8 - k contains forward - looking statements within the meaning of section 27 ##a of the securities act of 1933 , as amended , and section 21 ##e of the securities exchange act of 1934 , as amended , which are intended to be covered by the safe harbor created by such sections and other applicable laws and forward - looking information within the meaning of applicable canadian securities laws . where a forward - looking statement expresses or implies an expectation or belief as to future events or results , such expectation or belief is expressed in good faith and believed to have a reasonable basis . however , such statements are subject to risks , uncertain ##ties and other factors [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1018 1010 10476 1010 2047 9629 5471 3840 1006 1996 2194 2030 2047 9629 1007 2623 2008 15337 1997 2751 24586 4297 1012 1006 2751 24586 1007 4844 2019 6512 2000 11506 2007 2047 9629 1006 1996 3818 2047 9629 2751 24586 12598 1007 2012 1037 2569 3116 1997 15337 2218 2006 2258 1018 1010 10476 1012 2751 24586 2623 2008 15337 5444 24783 1999 5684 1997 11566 2007 2047 9629 1010 2007 2062 2084 5989 3867 1997 4494 3459 1999 5684 1997 1996 3818 2047 9629 2751 24586 12598 1012 1037 6100 1997 2047 9629 2015 2811 2713 2003 4987 2004 8327 5585 1012 1015 1998 2003 5100 2046 2023 8875 1022 1012 5890 2011 4431 1012 14046 5649 4861 4953 2830 1011 2559 8635 2023 2783 3189 2006 2433 1022 1011 1047 3397 2830 1011 2559 8635 2306 1996 3574 1997 2930 2676 2050 1997 1996 12012 2552 1997 4537 1010 2004 13266 1010 1998 2930 2538 2063 1997 1996 12012 3863 2552 1997 4579 1010 2004 13266 1010 2029 2024 3832 2000 2022 3139 2011 1996 3647 6496 2580 2011 2107 5433 1998 2060 12711 4277 1998 2830 1011 2559 2592 2306 1996 3574 1997 12711 3010 12012 4277 1012 2073 1037 2830 1011 2559 4861 16783 2030 12748 2019 17626 2030 6772 2004 2000 2925 2824 2030 3463 1010 2107 17626 2030 6772 2003 5228 1999 2204 4752 1998 3373 2000 2031 1037 9608 3978 1012 2174 1010 2107 8635 2024 3395 2000 10831 1010 9662 7368 1998 2060 5876 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1018 1010 10476 1010 2047 9629 5471 3840 1006 1996 2194 2030 2047 9629 1007 2623 2008 15337 1997 2751 24586 4297 1012 1006 2751 24586 1007 4844 2019 6512 2000 11506 2007 2047 9629 1006 1996 3818 2047 9629 2751 24586 12598 1007 2012 1037 2569 3116 1997 15337 2218 2006 2258 1018 1010 10476 1012 2751 24586 2623 2008 15337 5444 24783 1999 5684 1997 11566 2007 2047 9629 1010 2007 2062 2084 5989 3867 1997 4494 3459 1999 5684 1997 1996 3818 2047 9629 2751 24586 12598 1012 1037 6100 1997 2047 9629 2015 2811 2713 2003 4987 2004 8327 5585 1012 1015 1998 2003 5100 2046 2023 8875 1022 1012 5890 2011 4431 1012 14046 5649 4861 4953 2830 1011 2559 8635 2023 2783 3189 2006 2433 1022 1011 1047 3397 2830 1011 2559 8635 2306 1996 3574 1997 2930 2676 2050 1997 1996 12012 2552 1997 4537 1010 2004 13266 1010 1998 2930 2538 2063 1997 1996 12012 3863 2552 1997 4579 1010 2004 13266 1010 2029 2024 3832 2000 2022 3139 2011 1996 3647 6496 2580 2011 2107 5433 1998 2060 12711 4277 1998 2830 1011 2559 2592 2306 1996 3574 1997 12711 3010 12012 4277 1012 2073 1037 2830 1011 2559 4861 16783 2030 12748 2019 17626 2030 6772 2004 2000 2925 2824 2030 3463 1010 2107 17626 2030 6772 2003 5228 1999 2204 4752 1998 3373 2000 2031 1037 9608 3978 1012 2174 1010 2107 8635 2024 3395 2000 10831 1010 9662 7368 1998 2060 5876 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: Neutral (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: Neutral (id = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] april 25 , 2018 , the corporation ’ s stock ##holders approved an amendment to the corporation ’ s rest ##ated certificate of incorporation ( the \" rest ##ated certificate of incorporation \" ) to eliminate the super ##ma ##jo ##rity voting requirement applicable to by ##law amendments . accordingly , on may 1 , 2018 , the corporation filed with the secretary of state of the state of delaware a certificate of amendment to the rest ##ated certificate of incorporation reflecting the amendment provided in appendix i to the corporation ’ s definitive proxy statement on schedule 14 ##a filed with the securities and exchange commission on march 15 , 2018 . the corporation also amended and rest ##ated its amended and rest ##ated by ##law ##s ( the “ by ##law ##s ” ) to conform with the amendment to the rest ##ated certificate of incorporation . the description of the newly amended and rest ##ated by ##law ##s is qualified in its entirety by the text of the amended and rest ##ated by ##law ##s , which is filed as exhibits 3 . 1 to this current report on form 8 - k and is incorporated by reference here ##in . the corporation ' s 2018 annual meeting was [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] april 25 , 2018 , the corporation ’ s stock ##holders approved an amendment to the corporation ’ s rest ##ated certificate of incorporation ( the \" rest ##ated certificate of incorporation \" ) to eliminate the super ##ma ##jo ##rity voting requirement applicable to by ##law amendments . accordingly , on may 1 , 2018 , the corporation filed with the secretary of state of the state of delaware a certificate of amendment to the rest ##ated certificate of incorporation reflecting the amendment provided in appendix i to the corporation ’ s definitive proxy statement on schedule 14 ##a filed with the securities and exchange commission on march 15 , 2018 . the corporation also amended and rest ##ated its amended and rest ##ated by ##law ##s ( the “ by ##law ##s ” ) to conform with the amendment to the rest ##ated certificate of incorporation . the description of the newly amended and rest ##ated by ##law ##s is qualified in its entirety by the text of the amended and rest ##ated by ##law ##s , which is filed as exhibits 3 . 1 to this current report on form 8 - k and is incorporated by reference here ##in . the corporation ' s 2018 annual meeting was [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2258 2423 1010 2760 1010 1996 3840 1521 1055 4518 17794 4844 2019 7450 2000 1996 3840 1521 1055 2717 4383 8196 1997 16935 1006 1996 1000 2717 4383 8196 1997 16935 1000 1007 2000 11027 1996 3565 2863 5558 15780 6830 9095 12711 2000 2011 14919 16051 1012 11914 1010 2006 2089 1015 1010 2760 1010 1996 3840 6406 2007 1996 3187 1997 2110 1997 1996 2110 1997 8452 1037 8196 1997 7450 2000 1996 2717 4383 8196 1997 16935 10842 1996 7450 3024 1999 22524 1045 2000 1996 3840 1521 1055 15764 24540 4861 2006 6134 2403 2050 6406 2007 1996 12012 1998 3863 3222 2006 2233 2321 1010 2760 1012 1996 3840 2036 13266 1998 2717 4383 2049 13266 1998 2717 4383 2011 14919 2015 1006 1996 1523 2011 14919 2015 1524 1007 2000 23758 2007 1996 7450 2000 1996 2717 4383 8196 1997 16935 1012 1996 6412 1997 1996 4397 13266 1998 2717 4383 2011 14919 2015 2003 4591 1999 2049 15700 2011 1996 3793 1997 1996 13266 1998 2717 4383 2011 14919 2015 1010 2029 2003 6406 2004 10637 1017 1012 1015 2000 2023 2783 3189 2006 2433 1022 1011 1047 1998 2003 5100 2011 4431 2182 2378 1012 1996 3840 1005 1055 2760 3296 3116 2001 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2258 2423 1010 2760 1010 1996 3840 1521 1055 4518 17794 4844 2019 7450 2000 1996 3840 1521 1055 2717 4383 8196 1997 16935 1006 1996 1000 2717 4383 8196 1997 16935 1000 1007 2000 11027 1996 3565 2863 5558 15780 6830 9095 12711 2000 2011 14919 16051 1012 11914 1010 2006 2089 1015 1010 2760 1010 1996 3840 6406 2007 1996 3187 1997 2110 1997 1996 2110 1997 8452 1037 8196 1997 7450 2000 1996 2717 4383 8196 1997 16935 10842 1996 7450 3024 1999 22524 1045 2000 1996 3840 1521 1055 15764 24540 4861 2006 6134 2403 2050 6406 2007 1996 12012 1998 3863 3222 2006 2233 2321 1010 2760 1012 1996 3840 2036 13266 1998 2717 4383 2049 13266 1998 2717 4383 2011 14919 2015 1006 1996 1523 2011 14919 2015 1524 1007 2000 23758 2007 1996 7450 2000 1996 2717 4383 8196 1997 16935 1012 1996 6412 1997 1996 4397 13266 1998 2717 4383 2011 14919 2015 2003 4591 1999 2049 15700 2011 1996 3793 1997 1996 13266 1998 2717 4383 2011 14919 2015 1010 2029 2003 6406 2004 10637 1017 1012 1015 2000 2023 2783 3189 2006 2433 1022 1011 1047 1998 2003 5100 2011 4431 2182 2378 1012 1996 3840 1005 1055 2760 3296 3116 2001 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: Positive (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: Positive (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] g ##d ) today reported third - quarter 2018 earnings from continuing operations of $ 86 ##4 million , a 13 . 1 percent increase over third - quarter 2017 . revenue increased 20 percent to $ 9 . 1 billion . while a large portion of the growth was attributed to the acquisition of cs ##ra , revenue in all segments grew . dil ##uted earnings per share ( eps ) from continuing operations were $ 2 . 89 compared to $ 2 . 52 in the year - ago quarter , a 14 . 7 percent increase . “ we took action this quarter to stream ##line our portfolio , drive out risk from our supply chain and deliver increasingly sophisticated products and services to our customers in an efficient and timely manner , ” said ph ##eb ##e novak ##ovic , chairman and chief executive officer . “ we remain committed to generating steady and sustainable results from our businesses . ” significant activities this quarter included the delivery of the virginia - class submarine ss ##n 79 ##0 ( future uss south dakota ) , the keel - laying of the first john lewis - class fleet rep ##len ##ishment oil ##er and the continued integration of cs ##ra . margin ##com ##pan ##y - wide operating margin for the third quarter of 2018 was 12 . 5 percent , a 70 basis - point increase over second - quarter 2018 . cash ##net cash provided by operating activities in [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] g ##d ) today reported third - quarter 2018 earnings from continuing operations of $ 86 ##4 million , a 13 . 1 percent increase over third - quarter 2017 . revenue increased 20 percent to $ 9 . 1 billion . while a large portion of the growth was attributed to the acquisition of cs ##ra , revenue in all segments grew . dil ##uted earnings per share ( eps ) from continuing operations were $ 2 . 89 compared to $ 2 . 52 in the year - ago quarter , a 14 . 7 percent increase . “ we took action this quarter to stream ##line our portfolio , drive out risk from our supply chain and deliver increasingly sophisticated products and services to our customers in an efficient and timely manner , ” said ph ##eb ##e novak ##ovic , chairman and chief executive officer . “ we remain committed to generating steady and sustainable results from our businesses . ” significant activities this quarter included the delivery of the virginia - class submarine ss ##n 79 ##0 ( future uss south dakota ) , the keel - laying of the first john lewis - class fleet rep ##len ##ishment oil ##er and the continued integration of cs ##ra . margin ##com ##pan ##y - wide operating margin for the third quarter of 2018 was 12 . 5 percent , a 70 basis - point increase over second - quarter 2018 . cash ##net cash provided by operating activities in [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1043 2094 1007 2651 2988 2353 1011 4284 2760 16565 2013 5719 3136 1997 1002 6564 2549 2454 1010 1037 2410 1012 1015 3867 3623 2058 2353 1011 4284 2418 1012 6599 3445 2322 3867 2000 1002 1023 1012 1015 4551 1012 2096 1037 2312 4664 1997 1996 3930 2001 7108 2000 1996 7654 1997 20116 2527 1010 6599 1999 2035 9214 3473 1012 29454 12926 16565 2566 3745 1006 20383 1007 2013 5719 3136 2020 1002 1016 1012 6486 4102 2000 1002 1016 1012 4720 1999 1996 2095 1011 3283 4284 1010 1037 2403 1012 1021 3867 3623 1012 1523 2057 2165 2895 2023 4284 2000 5460 4179 2256 11103 1010 3298 2041 3891 2013 2256 4425 4677 1998 8116 6233 12138 3688 1998 2578 2000 2256 6304 1999 2019 8114 1998 23259 5450 1010 1524 2056 6887 15878 2063 19580 9142 1010 3472 1998 2708 3237 2961 1012 1523 2057 3961 5462 2000 11717 6706 1998 9084 3463 2013 2256 5661 1012 1524 3278 3450 2023 4284 2443 1996 6959 1997 1996 3448 1011 2465 6982 7020 2078 6535 2692 1006 2925 7234 2148 7734 1007 1010 1996 19602 1011 10201 1997 1996 2034 2198 4572 1011 2465 4170 16360 7770 21808 3514 2121 1998 1996 2506 8346 1997 20116 2527 1012 7785 9006 9739 2100 1011 2898 4082 7785 2005 1996 2353 4284 1997 2760 2001 2260 1012 1019 3867 1010 1037 3963 3978 1011 2391 3623 2058 2117 1011 4284 2760 1012 5356 7159 5356 3024 2011 4082 3450 1999 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1043 2094 1007 2651 2988 2353 1011 4284 2760 16565 2013 5719 3136 1997 1002 6564 2549 2454 1010 1037 2410 1012 1015 3867 3623 2058 2353 1011 4284 2418 1012 6599 3445 2322 3867 2000 1002 1023 1012 1015 4551 1012 2096 1037 2312 4664 1997 1996 3930 2001 7108 2000 1996 7654 1997 20116 2527 1010 6599 1999 2035 9214 3473 1012 29454 12926 16565 2566 3745 1006 20383 1007 2013 5719 3136 2020 1002 1016 1012 6486 4102 2000 1002 1016 1012 4720 1999 1996 2095 1011 3283 4284 1010 1037 2403 1012 1021 3867 3623 1012 1523 2057 2165 2895 2023 4284 2000 5460 4179 2256 11103 1010 3298 2041 3891 2013 2256 4425 4677 1998 8116 6233 12138 3688 1998 2578 2000 2256 6304 1999 2019 8114 1998 23259 5450 1010 1524 2056 6887 15878 2063 19580 9142 1010 3472 1998 2708 3237 2961 1012 1523 2057 3961 5462 2000 11717 6706 1998 9084 3463 2013 2256 5661 1012 1524 3278 3450 2023 4284 2443 1996 6959 1997 1996 3448 1011 2465 6982 7020 2078 6535 2692 1006 2925 7234 2148 7734 1007 1010 1996 19602 1011 10201 1997 1996 2034 2198 4572 1011 2465 4170 16360 7770 21808 3514 2121 1998 1996 2506 8346 1997 20116 2527 1012 7785 9006 9739 2100 1011 2898 4082 7785 2005 1996 2353 4284 1997 2760 2001 2260 1012 1019 3867 1010 1037 3963 3978 1011 2391 3623 2058 2117 1011 4284 2760 1012 5356 7159 5356 3024 2011 4082 3450 1999 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: Negative (id = 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: Negative (id = 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] 22 , 2018 , the hartford financial services group , inc . ( the “ company ” ) filed a current report on form 8 - k ( the “ original report ” ) disc ##los ##ing that it had entered into : ( i ) an agreement and plan of merger ( the “ merger agreement ” ) with the navigator ##s group , inc . and ren ##ato acquisition co . , a direct wholly owned subsidiary of the company ; ( ii ) a voting agreement ( the “ dee ##ks voting agreement ” ) with terence n . dee ##ks , monica j . dee ##ks , the dee ##ks family foundation and certain trusts for the benefit of members of the dee ##ks family ; and ( iii ) a voting agreement ( the “ gala ##nsk ##i voting agreement ” and , together with the dee ##ks voting agreement , the “ voting agreements ” ) with stanley a . gala ##nsk ##i . this form 8 - k / a amend ##s the original report to include the merger agreement , which is filed as exhibit 2 . 1 here ##to and is incorporated here ##in by reference , and the voting agreements , which are filed as exhibit 99 . 2 and exhibit 99 . 3 here ##to and are incorporated here ##in by reference . the merger agreement has been included solely to provide investors and security holders with information regarding its terms . it [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] 22 , 2018 , the hartford financial services group , inc . ( the “ company ” ) filed a current report on form 8 - k ( the “ original report ” ) disc ##los ##ing that it had entered into : ( i ) an agreement and plan of merger ( the “ merger agreement ” ) with the navigator ##s group , inc . and ren ##ato acquisition co . , a direct wholly owned subsidiary of the company ; ( ii ) a voting agreement ( the “ dee ##ks voting agreement ” ) with terence n . dee ##ks , monica j . dee ##ks , the dee ##ks family foundation and certain trusts for the benefit of members of the dee ##ks family ; and ( iii ) a voting agreement ( the “ gala ##nsk ##i voting agreement ” and , together with the dee ##ks voting agreement , the “ voting agreements ” ) with stanley a . gala ##nsk ##i . this form 8 - k / a amend ##s the original report to include the merger agreement , which is filed as exhibit 2 . 1 here ##to and is incorporated here ##in by reference , and the voting agreements , which are filed as exhibit 99 . 2 and exhibit 99 . 3 here ##to and are incorporated here ##in by reference . the merger agreement has been included solely to provide investors and security holders with information regarding its terms . it [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2570 1010 2760 1010 1996 13381 3361 2578 2177 1010 4297 1012 1006 1996 1523 2194 1524 1007 6406 1037 2783 3189 2006 2433 1022 1011 1047 1006 1996 1523 2434 3189 1524 1007 5860 10483 2075 2008 2009 2018 3133 2046 1024 1006 1045 1007 2019 3820 1998 2933 1997 7660 1006 1996 1523 7660 3820 1524 1007 2007 1996 20532 2015 2177 1010 4297 1012 1998 14916 10610 7654 2522 1012 1010 1037 3622 12590 3079 7506 1997 1996 2194 1025 1006 2462 1007 1037 6830 3820 1006 1996 1523 9266 5705 6830 3820 1524 1007 2007 22677 1050 1012 9266 5705 1010 9018 1046 1012 9266 5705 1010 1996 9266 5705 2155 3192 1998 3056 20278 2005 1996 5770 1997 2372 1997 1996 9266 5705 2155 1025 1998 1006 3523 1007 1037 6830 3820 1006 1996 1523 16122 25564 2072 6830 3820 1524 1998 1010 2362 2007 1996 9266 5705 6830 3820 1010 1996 1523 6830 10540 1524 1007 2007 6156 1037 1012 16122 25564 2072 1012 2023 2433 1022 1011 1047 1013 1037 27950 2015 1996 2434 3189 2000 2421 1996 7660 3820 1010 2029 2003 6406 2004 8327 1016 1012 1015 2182 3406 1998 2003 5100 2182 2378 2011 4431 1010 1998 1996 6830 10540 1010 2029 2024 6406 2004 8327 5585 1012 1016 1998 8327 5585 1012 1017 2182 3406 1998 2024 5100 2182 2378 2011 4431 1012 1996 7660 3820 2038 2042 2443 9578 2000 3073 9387 1998 3036 13304 2007 2592 4953 2049 3408 1012 2009 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2570 1010 2760 1010 1996 13381 3361 2578 2177 1010 4297 1012 1006 1996 1523 2194 1524 1007 6406 1037 2783 3189 2006 2433 1022 1011 1047 1006 1996 1523 2434 3189 1524 1007 5860 10483 2075 2008 2009 2018 3133 2046 1024 1006 1045 1007 2019 3820 1998 2933 1997 7660 1006 1996 1523 7660 3820 1524 1007 2007 1996 20532 2015 2177 1010 4297 1012 1998 14916 10610 7654 2522 1012 1010 1037 3622 12590 3079 7506 1997 1996 2194 1025 1006 2462 1007 1037 6830 3820 1006 1996 1523 9266 5705 6830 3820 1524 1007 2007 22677 1050 1012 9266 5705 1010 9018 1046 1012 9266 5705 1010 1996 9266 5705 2155 3192 1998 3056 20278 2005 1996 5770 1997 2372 1997 1996 9266 5705 2155 1025 1998 1006 3523 1007 1037 6830 3820 1006 1996 1523 16122 25564 2072 6830 3820 1524 1998 1010 2362 2007 1996 9266 5705 6830 3820 1010 1996 1523 6830 10540 1524 1007 2007 6156 1037 1012 16122 25564 2072 1012 2023 2433 1022 1011 1047 1013 1037 27950 2015 1996 2434 3189 2000 2421 1996 7660 3820 1010 2029 2003 6406 2004 8327 1016 1012 1015 2182 3406 1998 2003 5100 2182 2378 2011 4431 1010 1998 1996 6830 10540 1010 2029 2024 6406 2004 8327 5585 1012 1016 1998 8327 5585 1012 1017 2182 3406 1998 2024 5100 2182 2378 2011 4431 1012 1996 7660 3820 2038 2042 2443 9578 2000 3073 9387 1998 3036 13304 2007 2592 4953 2049 3408 1012 2009 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: Neutral (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: Neutral (id = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 10000 of 10071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 10000 of 10071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] a ) today reported revenue of $ 1 . 36 billion for the first quarter ended jan . 31 , 2020 , up 5 . 7 % year over year ( and up 2 . 4 % on a core ( 1 ) basis ) . on a gaa ##p basis , first - quarter net income was $ 197 million or 63 cents per share . this compares with $ 50 ##4 million or $ 1 . 57 per share in the first quarter of fiscal year 2019 . gaa ##p earnings per share were down 60 % year over year . non - gaa ##p ( 2 ) net income was $ 252 million or 81 cents per share during the quarter compared with $ 244 million or 76 cents per share during the first quarter a year ago . non - gaa ##p earnings per share were up 7 % year over year . “ the agile ##nt team delivered a strong start to 2020 . our revenues were above expectations with growth across all regions and markets , and earnings per share at the top end of our guidance , ” said mike mc ##mu ##llen , agile ##nt president and ceo . “ our first - quarter results provide clear evidence our growth strategy is working . ” financial highlights ##life sciences and applied markets group ##fi ##rst - quarter revenue of $ 63 ##8 million from agile ##nt ’ s life sciences and applied markets group ( l [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] a ) today reported revenue of $ 1 . 36 billion for the first quarter ended jan . 31 , 2020 , up 5 . 7 % year over year ( and up 2 . 4 % on a core ( 1 ) basis ) . on a gaa ##p basis , first - quarter net income was $ 197 million or 63 cents per share . this compares with $ 50 ##4 million or $ 1 . 57 per share in the first quarter of fiscal year 2019 . gaa ##p earnings per share were down 60 % year over year . non - gaa ##p ( 2 ) net income was $ 252 million or 81 cents per share during the quarter compared with $ 244 million or 76 cents per share during the first quarter a year ago . non - gaa ##p earnings per share were up 7 % year over year . “ the agile ##nt team delivered a strong start to 2020 . our revenues were above expectations with growth across all regions and markets , and earnings per share at the top end of our guidance , ” said mike mc ##mu ##llen , agile ##nt president and ceo . “ our first - quarter results provide clear evidence our growth strategy is working . ” financial highlights ##life sciences and applied markets group ##fi ##rst - quarter revenue of $ 63 ##8 million from agile ##nt ’ s life sciences and applied markets group ( l [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1037 1007 2651 2988 6599 1997 1002 1015 1012 4029 4551 2005 1996 2034 4284 3092 5553 1012 2861 1010 12609 1010 2039 1019 1012 1021 1003 2095 2058 2095 1006 1998 2039 1016 1012 1018 1003 2006 1037 4563 1006 1015 1007 3978 1007 1012 2006 1037 19930 2361 3978 1010 2034 1011 4284 5658 3318 2001 1002 19975 2454 2030 6191 16653 2566 3745 1012 2023 22963 2007 1002 2753 2549 2454 2030 1002 1015 1012 5401 2566 3745 1999 1996 2034 4284 1997 10807 2095 10476 1012 19930 2361 16565 2566 3745 2020 2091 3438 1003 2095 2058 2095 1012 2512 1011 19930 2361 1006 1016 1007 5658 3318 2001 1002 22898 2454 2030 6282 16653 2566 3745 2076 1996 4284 4102 2007 1002 24194 2454 2030 6146 16653 2566 3745 2076 1996 2034 4284 1037 2095 3283 1012 2512 1011 19930 2361 16565 2566 3745 2020 2039 1021 1003 2095 2058 2095 1012 1523 1996 29003 3372 2136 5359 1037 2844 2707 2000 12609 1012 2256 12594 2020 2682 10908 2007 3930 2408 2035 4655 1998 6089 1010 1998 16565 2566 3745 2012 1996 2327 2203 1997 2256 8606 1010 1524 2056 3505 11338 12274 12179 1010 29003 3372 2343 1998 5766 1012 1523 2256 2034 1011 4284 3463 3073 3154 3350 2256 3930 5656 2003 2551 1012 1524 3361 11637 15509 4163 1998 4162 6089 2177 8873 12096 1011 4284 6599 1997 1002 6191 2620 2454 2013 29003 3372 1521 1055 2166 4163 1998 4162 6089 2177 1006 1048 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1037 1007 2651 2988 6599 1997 1002 1015 1012 4029 4551 2005 1996 2034 4284 3092 5553 1012 2861 1010 12609 1010 2039 1019 1012 1021 1003 2095 2058 2095 1006 1998 2039 1016 1012 1018 1003 2006 1037 4563 1006 1015 1007 3978 1007 1012 2006 1037 19930 2361 3978 1010 2034 1011 4284 5658 3318 2001 1002 19975 2454 2030 6191 16653 2566 3745 1012 2023 22963 2007 1002 2753 2549 2454 2030 1002 1015 1012 5401 2566 3745 1999 1996 2034 4284 1997 10807 2095 10476 1012 19930 2361 16565 2566 3745 2020 2091 3438 1003 2095 2058 2095 1012 2512 1011 19930 2361 1006 1016 1007 5658 3318 2001 1002 22898 2454 2030 6282 16653 2566 3745 2076 1996 4284 4102 2007 1002 24194 2454 2030 6146 16653 2566 3745 2076 1996 2034 4284 1037 2095 3283 1012 2512 1011 19930 2361 16565 2566 3745 2020 2039 1021 1003 2095 2058 2095 1012 1523 1996 29003 3372 2136 5359 1037 2844 2707 2000 12609 1012 2256 12594 2020 2682 10908 2007 3930 2408 2035 4655 1998 6089 1010 1998 16565 2566 3745 2012 1996 2327 2203 1997 2256 8606 1010 1524 2056 3505 11338 12274 12179 1010 29003 3372 2343 1998 5766 1012 1523 2256 2034 1011 4284 3463 3073 3154 3350 2256 3930 5656 2003 2551 1012 1524 3361 11637 15509 4163 1998 4162 6089 2177 8873 12096 1011 4284 6599 1997 1002 6191 2620 2454 2013 29003 3372 1521 1055 2166 4163 1998 4162 6089 2177 1006 1048 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: Positive (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: Positive (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] a ) today reported revenue of $ 1 . 37 billion for the fourth quarter ended oct . 31 , 2019 , up 6 % year over year ( core ( 1 ) growth of 4 % ) . on a gaa ##p basis , fourth - quarter net income was $ 194 million , or 62 cents per share . this compares with $ 195 million , or 61 cents per share , in the fourth quarter of fiscal year 2018 . non - gaa ##p ( 2 ) net income was $ 277 million , or 89 cents per share , during the quarter , compared with $ 262 million or 81 cents per share during the fourth quarter a year ago . “ agile ##nt ’ s fourth - quarter results cap off a very solid 2019 and reflect the broad - based business we ’ ve built over the last five years , ” said mike mc ##mu ##llen , agile ##nt president and ceo . “ based on what we ’ ve been able to achieve in 2019 , i ’ m convinced we ’ re in an exceptionally strong position for the future . ” financial highlights ##life sciences and applied markets group ##1 ##fo ##urt ##h - quarter revenue of $ 62 ##2 million from agile ##nt ’ s life sciences and applied markets group ( l ##sa ##g ) grew a reported 4 % year over year ( declining 2 % on a core ( 1 [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] a ) today reported revenue of $ 1 . 37 billion for the fourth quarter ended oct . 31 , 2019 , up 6 % year over year ( core ( 1 ) growth of 4 % ) . on a gaa ##p basis , fourth - quarter net income was $ 194 million , or 62 cents per share . this compares with $ 195 million , or 61 cents per share , in the fourth quarter of fiscal year 2018 . non - gaa ##p ( 2 ) net income was $ 277 million , or 89 cents per share , during the quarter , compared with $ 262 million or 81 cents per share during the fourth quarter a year ago . “ agile ##nt ’ s fourth - quarter results cap off a very solid 2019 and reflect the broad - based business we ’ ve built over the last five years , ” said mike mc ##mu ##llen , agile ##nt president and ceo . “ based on what we ’ ve been able to achieve in 2019 , i ’ m convinced we ’ re in an exceptionally strong position for the future . ” financial highlights ##life sciences and applied markets group ##1 ##fo ##urt ##h - quarter revenue of $ 62 ##2 million from agile ##nt ’ s life sciences and applied markets group ( l ##sa ##g ) grew a reported 4 % year over year ( declining 2 % on a core ( 1 [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1037 1007 2651 2988 6599 1997 1002 1015 1012 4261 4551 2005 1996 2959 4284 3092 13323 1012 2861 1010 10476 1010 2039 1020 1003 2095 2058 2095 1006 4563 1006 1015 1007 3930 1997 1018 1003 1007 1012 2006 1037 19930 2361 3978 1010 2959 1011 4284 5658 3318 2001 1002 19955 2454 1010 2030 5786 16653 2566 3745 1012 2023 22963 2007 1002 17317 2454 1010 2030 6079 16653 2566 3745 1010 1999 1996 2959 4284 1997 10807 2095 2760 1012 2512 1011 19930 2361 1006 1016 1007 5658 3318 2001 1002 25578 2454 1010 2030 6486 16653 2566 3745 1010 2076 1996 4284 1010 4102 2007 1002 21950 2454 2030 6282 16653 2566 3745 2076 1996 2959 4284 1037 2095 3283 1012 1523 29003 3372 1521 1055 2959 1011 4284 3463 6178 2125 1037 2200 5024 10476 1998 8339 1996 5041 1011 2241 2449 2057 1521 2310 2328 2058 1996 2197 2274 2086 1010 1524 2056 3505 11338 12274 12179 1010 29003 3372 2343 1998 5766 1012 1523 2241 2006 2054 2057 1521 2310 2042 2583 2000 6162 1999 10476 1010 1045 1521 1049 6427 2057 1521 2128 1999 2019 17077 2844 2597 2005 1996 2925 1012 1524 3361 11637 15509 4163 1998 4162 6089 2177 2487 14876 19585 2232 1011 4284 6599 1997 1002 5786 2475 2454 2013 29003 3372 1521 1055 2166 4163 1998 4162 6089 2177 1006 1048 3736 2290 1007 3473 1037 2988 1018 1003 2095 2058 2095 1006 13993 1016 1003 2006 1037 4563 1006 1015 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1037 1007 2651 2988 6599 1997 1002 1015 1012 4261 4551 2005 1996 2959 4284 3092 13323 1012 2861 1010 10476 1010 2039 1020 1003 2095 2058 2095 1006 4563 1006 1015 1007 3930 1997 1018 1003 1007 1012 2006 1037 19930 2361 3978 1010 2959 1011 4284 5658 3318 2001 1002 19955 2454 1010 2030 5786 16653 2566 3745 1012 2023 22963 2007 1002 17317 2454 1010 2030 6079 16653 2566 3745 1010 1999 1996 2959 4284 1997 10807 2095 2760 1012 2512 1011 19930 2361 1006 1016 1007 5658 3318 2001 1002 25578 2454 1010 2030 6486 16653 2566 3745 1010 2076 1996 4284 1010 4102 2007 1002 21950 2454 2030 6282 16653 2566 3745 2076 1996 2959 4284 1037 2095 3283 1012 1523 29003 3372 1521 1055 2959 1011 4284 3463 6178 2125 1037 2200 5024 10476 1998 8339 1996 5041 1011 2241 2449 2057 1521 2310 2328 2058 1996 2197 2274 2086 1010 1524 2056 3505 11338 12274 12179 1010 29003 3372 2343 1998 5766 1012 1523 2241 2006 2054 2057 1521 2310 2042 2583 2000 6162 1999 10476 1010 1045 1521 1049 6427 2057 1521 2128 1999 2019 17077 2844 2597 2005 1996 2925 1012 1524 3361 11637 15509 4163 1998 4162 6089 2177 2487 14876 19585 2232 1011 4284 6599 1997 1002 5786 2475 2454 2013 29003 3372 1521 1055 2166 4163 1998 4162 6089 2177 1006 1048 3736 2290 1007 3473 1037 2988 1018 1003 2095 2058 2095 1006 13993 1016 1003 2006 1037 4563 1006 1015 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: Positive (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: Positive (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] the close of business on february 12 , 2020 . ( d ) exhibits : signatures pursuant to the requirements of the securities exchange act of 1934 , the regis ##tra ##nt has duly caused this report to be signed on its behalf by the under ##si ##gned here ##unt ##o duly authorized . ex - 99 . 1 2 d ##8 ##7 ##44 ##55 ##de ##x ##9 ##9 ##1 . h ##tm ex - 99 . 1 ex - 99 . 1 exhibit 99 . 1 market ##ax ##ess reports fourth quarter 2019 revenues of $ 129 . 8 million , operating income of $ 60 . 9 million and dil ##uted eps of $ 1 . 32 11 consecutive years of record volume , revenue and earnings company announces 18 % increase in regular quarterly divide ##nd to $ 0 . 60 per share , up from $ 0 . 51 fourth quarter financial highlights * [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] the close of business on february 12 , 2020 . ( d ) exhibits : signatures pursuant to the requirements of the securities exchange act of 1934 , the regis ##tra ##nt has duly caused this report to be signed on its behalf by the under ##si ##gned here ##unt ##o duly authorized . ex - 99 . 1 2 d ##8 ##7 ##44 ##55 ##de ##x ##9 ##9 ##1 . h ##tm ex - 99 . 1 ex - 99 . 1 exhibit 99 . 1 market ##ax ##ess reports fourth quarter 2019 revenues of $ 129 . 8 million , operating income of $ 60 . 9 million and dil ##uted eps of $ 1 . 32 11 consecutive years of record volume , revenue and earnings company announces 18 % increase in regular quarterly divide ##nd to $ 0 . 60 per share , up from $ 0 . 51 fourth quarter financial highlights * [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1996 2485 1997 2449 2006 2337 2260 1010 12609 1012 1006 1040 1007 10637 1024 16442 27081 2000 1996 5918 1997 1996 12012 3863 2552 1997 4579 1010 1996 20588 6494 3372 2038 25073 3303 2023 3189 2000 2022 2772 2006 2049 6852 2011 1996 2104 5332 19225 2182 16671 2080 25073 9362 1012 4654 1011 5585 1012 1015 1016 1040 2620 2581 22932 24087 3207 2595 2683 2683 2487 1012 1044 21246 4654 1011 5585 1012 1015 4654 1011 5585 1012 1015 8327 5585 1012 1015 3006 8528 7971 4311 2959 4284 10476 12594 1997 1002 14378 1012 1022 2454 1010 4082 3318 1997 1002 3438 1012 1023 2454 1998 29454 12926 20383 1997 1002 1015 1012 3590 2340 5486 2086 1997 2501 3872 1010 6599 1998 16565 2194 17472 2324 1003 3623 1999 3180 12174 11443 4859 2000 1002 1014 1012 3438 2566 3745 1010 2039 2013 1002 1014 1012 4868 2959 4284 3361 11637 1008 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1996 2485 1997 2449 2006 2337 2260 1010 12609 1012 1006 1040 1007 10637 1024 16442 27081 2000 1996 5918 1997 1996 12012 3863 2552 1997 4579 1010 1996 20588 6494 3372 2038 25073 3303 2023 3189 2000 2022 2772 2006 2049 6852 2011 1996 2104 5332 19225 2182 16671 2080 25073 9362 1012 4654 1011 5585 1012 1015 1016 1040 2620 2581 22932 24087 3207 2595 2683 2683 2487 1012 1044 21246 4654 1011 5585 1012 1015 4654 1011 5585 1012 1015 8327 5585 1012 1015 3006 8528 7971 4311 2959 4284 10476 12594 1997 1002 14378 1012 1022 2454 1010 4082 3318 1997 1002 3438 1012 1023 2454 1998 29454 12926 20383 1997 1002 1015 1012 3590 2340 5486 2086 1997 2501 3872 1010 6599 1998 16565 2194 17472 2324 1003 3623 1999 3180 12174 11443 4859 2000 1002 1014 1012 3438 2566 3745 1010 2039 2013 1002 1014 1012 4868 2959 4284 3361 11637 1008 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: Negative (id = 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: Negative (id = 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] 3 , 2020 , carlos abrams - rivera , executive vice president and president , campbell snacks of campbell soup company ( the “ company ” ) , informed the company that he is resigning effective february 1 , 2020 . signatures ##pur ##su ##ant to the requirements of the securities exchange act of 1934 , as amended , the regis ##tra ##nt has duly caused this report to be signed on its behalf by the under ##si ##gned here ##unt ##o duly authorized . ex - 101 . sc ##h 2 cp ##b - 2020 ##01 ##0 ##3 . x ##sd x ##br ##l taxonomy extension sc ##hema document 000 ##100 ##1 - document - cover document link : presentation ##link link : calculation ##link link : definition ##link ex - 101 . cal 3 cp ##b - 2020 ##01 ##0 ##3 _ cal . xml x ##br ##l taxonomy extension calculation link ##base document ex - 101 . def 4 cp ##b - 2020 ##01 ##0 ##3 _ def . xml x ##br ##l taxonomy extension definition link ##base document ex - 101 . lab 5 cp ##b - 2020 ##01 ##0 ##3 _ lab . xml x ##br ##l taxonomy extension label link ##base document amendment flag amendment flag document period end date document period end date document type document type cover page . cover page . city area code city area code entity emerging growth company entity emerging growth company entity file number entity file number entity address , postal [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] 3 , 2020 , carlos abrams - rivera , executive vice president and president , campbell snacks of campbell soup company ( the “ company ” ) , informed the company that he is resigning effective february 1 , 2020 . signatures ##pur ##su ##ant to the requirements of the securities exchange act of 1934 , as amended , the regis ##tra ##nt has duly caused this report to be signed on its behalf by the under ##si ##gned here ##unt ##o duly authorized . ex - 101 . sc ##h 2 cp ##b - 2020 ##01 ##0 ##3 . x ##sd x ##br ##l taxonomy extension sc ##hema document 000 ##100 ##1 - document - cover document link : presentation ##link link : calculation ##link link : definition ##link ex - 101 . cal 3 cp ##b - 2020 ##01 ##0 ##3 _ cal . xml x ##br ##l taxonomy extension calculation link ##base document ex - 101 . def 4 cp ##b - 2020 ##01 ##0 ##3 _ def . xml x ##br ##l taxonomy extension definition link ##base document ex - 101 . lab 5 cp ##b - 2020 ##01 ##0 ##3 _ lab . xml x ##br ##l taxonomy extension label link ##base document amendment flag amendment flag document period end date document period end date document type document type cover page . cover page . city area code city area code entity emerging growth company entity emerging growth company entity file number entity file number entity address , postal [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1017 1010 12609 1010 5828 23063 1011 14043 1010 3237 3580 2343 1998 2343 1010 6063 27962 1997 6063 11350 2194 1006 1996 1523 2194 1524 1007 1010 6727 1996 2194 2008 2002 2003 24642 4621 2337 1015 1010 12609 1012 16442 5311 6342 4630 2000 1996 5918 1997 1996 12012 3863 2552 1997 4579 1010 2004 13266 1010 1996 20588 6494 3372 2038 25073 3303 2023 3189 2000 2022 2772 2006 2049 6852 2011 1996 2104 5332 19225 2182 16671 2080 25073 9362 1012 4654 1011 7886 1012 8040 2232 1016 18133 2497 1011 12609 24096 2692 2509 1012 1060 16150 1060 19892 2140 25274 5331 8040 28433 6254 2199 18613 2487 1011 6254 1011 3104 6254 4957 1024 8312 13767 4957 1024 17208 13767 4957 1024 6210 13767 4654 1011 7886 1012 10250 1017 18133 2497 1011 12609 24096 2692 2509 1035 10250 1012 20950 1060 19892 2140 25274 5331 17208 4957 15058 6254 4654 1011 7886 1012 13366 1018 18133 2497 1011 12609 24096 2692 2509 1035 13366 1012 20950 1060 19892 2140 25274 5331 6210 4957 15058 6254 4654 1011 7886 1012 6845 1019 18133 2497 1011 12609 24096 2692 2509 1035 6845 1012 20950 1060 19892 2140 25274 5331 3830 4957 15058 6254 7450 5210 7450 5210 6254 2558 2203 3058 6254 2558 2203 3058 6254 2828 6254 2828 3104 3931 1012 3104 3931 1012 2103 2181 3642 2103 2181 3642 9178 8361 3930 2194 9178 8361 3930 2194 9178 5371 2193 9178 5371 2193 9178 4769 1010 10690 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1017 1010 12609 1010 5828 23063 1011 14043 1010 3237 3580 2343 1998 2343 1010 6063 27962 1997 6063 11350 2194 1006 1996 1523 2194 1524 1007 1010 6727 1996 2194 2008 2002 2003 24642 4621 2337 1015 1010 12609 1012 16442 5311 6342 4630 2000 1996 5918 1997 1996 12012 3863 2552 1997 4579 1010 2004 13266 1010 1996 20588 6494 3372 2038 25073 3303 2023 3189 2000 2022 2772 2006 2049 6852 2011 1996 2104 5332 19225 2182 16671 2080 25073 9362 1012 4654 1011 7886 1012 8040 2232 1016 18133 2497 1011 12609 24096 2692 2509 1012 1060 16150 1060 19892 2140 25274 5331 8040 28433 6254 2199 18613 2487 1011 6254 1011 3104 6254 4957 1024 8312 13767 4957 1024 17208 13767 4957 1024 6210 13767 4654 1011 7886 1012 10250 1017 18133 2497 1011 12609 24096 2692 2509 1035 10250 1012 20950 1060 19892 2140 25274 5331 17208 4957 15058 6254 4654 1011 7886 1012 13366 1018 18133 2497 1011 12609 24096 2692 2509 1035 13366 1012 20950 1060 19892 2140 25274 5331 6210 4957 15058 6254 4654 1011 7886 1012 6845 1019 18133 2497 1011 12609 24096 2692 2509 1035 6845 1012 20950 1060 19892 2140 25274 5331 3830 4957 15058 6254 7450 5210 7450 5210 6254 2558 2203 3058 6254 2558 2203 3058 6254 2828 6254 2828 3104 3931 1012 3104 3931 1012 2103 2181 3642 2103 2181 3642 9178 8361 3930 2194 9178 8361 3930 2194 9178 5371 2193 9178 5371 2193 9178 4769 1010 10690 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: Negative (id = 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: Negative (id = 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] 4 , 2019 , campbell soup company issued a press release announcing financial results for the quarter ended october 27 , 2019 , a copy of which is attached as exhibit 99 . 1 . the information in this item 2 . 02 and exhibit 99 . 1 attached here ##to shall not be deemed “ filed ” for purposes of section 18 of the securities exchange act of 1934 , as amended , or otherwise subject to the lia ##bilities of that section , nor shall it be deemed incorporated by reference into any filing under the securities act of 1933 , as amended , except as express ##ly set forth by specific reference in such filing . item 9 . 01 – financial statements and exhibits ( d ) exhibits ##ex ##hi ##bit index ##ex ##hi ##bit no . signatures ##pur ##su ##ant to the requirements of the securities exchange act of 1934 , as amended , the regis ##tra ##nt has duly caused this report to be signed on its behalf by the under ##si ##gned here ##unt ##o duly authorized . ex - 99 . 1 2 exhibit ##9 ##9 ##1 - q ##12 ##0 ##20 . h ##tm ex - 99 . 1 document ##ex ##hi ##bit 99 . 1 ##for immediate release ##camp ##bell reports first - quarter results • earnings per share ( eps ) from continuing operations of $ 0 . 56 decreased 7 % primarily reflecting charges associated with sale of the european chips business [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] 4 , 2019 , campbell soup company issued a press release announcing financial results for the quarter ended october 27 , 2019 , a copy of which is attached as exhibit 99 . 1 . the information in this item 2 . 02 and exhibit 99 . 1 attached here ##to shall not be deemed “ filed ” for purposes of section 18 of the securities exchange act of 1934 , as amended , or otherwise subject to the lia ##bilities of that section , nor shall it be deemed incorporated by reference into any filing under the securities act of 1933 , as amended , except as express ##ly set forth by specific reference in such filing . item 9 . 01 – financial statements and exhibits ( d ) exhibits ##ex ##hi ##bit index ##ex ##hi ##bit no . signatures ##pur ##su ##ant to the requirements of the securities exchange act of 1934 , as amended , the regis ##tra ##nt has duly caused this report to be signed on its behalf by the under ##si ##gned here ##unt ##o duly authorized . ex - 99 . 1 2 exhibit ##9 ##9 ##1 - q ##12 ##0 ##20 . h ##tm ex - 99 . 1 document ##ex ##hi ##bit 99 . 1 ##for immediate release ##camp ##bell reports first - quarter results • earnings per share ( eps ) from continuing operations of $ 0 . 56 decreased 7 % primarily reflecting charges associated with sale of the european chips business [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1018 1010 10476 1010 6063 11350 2194 3843 1037 2811 2713 13856 3361 3463 2005 1996 4284 3092 2255 2676 1010 10476 1010 1037 6100 1997 2029 2003 4987 2004 8327 5585 1012 1015 1012 1996 2592 1999 2023 8875 1016 1012 6185 1998 8327 5585 1012 1015 4987 2182 3406 4618 2025 2022 8357 1523 6406 1524 2005 5682 1997 2930 2324 1997 1996 12012 3863 2552 1997 4579 1010 2004 13266 1010 2030 4728 3395 2000 1996 22393 14680 1997 2008 2930 1010 4496 4618 2009 2022 8357 5100 2011 4431 2046 2151 15242 2104 1996 12012 2552 1997 4537 1010 2004 13266 1010 3272 2004 4671 2135 2275 5743 2011 3563 4431 1999 2107 15242 1012 8875 1023 1012 5890 1516 3361 8635 1998 10637 1006 1040 1007 10637 10288 4048 16313 5950 10288 4048 16313 2053 1012 16442 5311 6342 4630 2000 1996 5918 1997 1996 12012 3863 2552 1997 4579 1010 2004 13266 1010 1996 20588 6494 3372 2038 25073 3303 2023 3189 2000 2022 2772 2006 2049 6852 2011 1996 2104 5332 19225 2182 16671 2080 25073 9362 1012 4654 1011 5585 1012 1015 1016 8327 2683 2683 2487 1011 1053 12521 2692 11387 1012 1044 21246 4654 1011 5585 1012 1015 6254 10288 4048 16313 5585 1012 1015 29278 6234 2713 26468 17327 4311 2034 1011 4284 3463 1528 16565 2566 3745 1006 20383 1007 2013 5719 3136 1997 1002 1014 1012 5179 10548 1021 1003 3952 10842 5571 3378 2007 5096 1997 1996 2647 11772 2449 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1018 1010 10476 1010 6063 11350 2194 3843 1037 2811 2713 13856 3361 3463 2005 1996 4284 3092 2255 2676 1010 10476 1010 1037 6100 1997 2029 2003 4987 2004 8327 5585 1012 1015 1012 1996 2592 1999 2023 8875 1016 1012 6185 1998 8327 5585 1012 1015 4987 2182 3406 4618 2025 2022 8357 1523 6406 1524 2005 5682 1997 2930 2324 1997 1996 12012 3863 2552 1997 4579 1010 2004 13266 1010 2030 4728 3395 2000 1996 22393 14680 1997 2008 2930 1010 4496 4618 2009 2022 8357 5100 2011 4431 2046 2151 15242 2104 1996 12012 2552 1997 4537 1010 2004 13266 1010 3272 2004 4671 2135 2275 5743 2011 3563 4431 1999 2107 15242 1012 8875 1023 1012 5890 1516 3361 8635 1998 10637 1006 1040 1007 10637 10288 4048 16313 5950 10288 4048 16313 2053 1012 16442 5311 6342 4630 2000 1996 5918 1997 1996 12012 3863 2552 1997 4579 1010 2004 13266 1010 1996 20588 6494 3372 2038 25073 3303 2023 3189 2000 2022 2772 2006 2049 6852 2011 1996 2104 5332 19225 2182 16671 2080 25073 9362 1012 4654 1011 5585 1012 1015 1016 8327 2683 2683 2487 1011 1053 12521 2692 11387 1012 1044 21246 4654 1011 5585 1012 1015 6254 10288 4048 16313 5585 1012 1015 29278 6234 2713 26468 17327 4311 2034 1011 4284 3463 1528 16565 2566 3745 1006 20383 1007 2013 5719 3136 1997 1002 1014 1012 5179 10548 1021 1003 3952 10842 5571 3378 2007 5096 1997 1996 2647 11772 2449 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: Positive (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: Positive (id = 0)\n"
     ]
    }
   ],
   "source": [
    "# Convert our train and test features to InputFeatures that BERT understands.\n",
    "train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n",
    "                 num_labels):\n",
    "#  \"\"\"Creates a classification model.\"\"\"\n",
    "    bert_module = hub.Module(\n",
    "      BERT_MODEL_HUB,\n",
    "      trainable=True)\n",
    "    bert_inputs = dict(\n",
    "      input_ids=input_ids,\n",
    "      input_mask=input_mask,\n",
    "      segment_ids=segment_ids)\n",
    "    bert_outputs = bert_module(\n",
    "      inputs=bert_inputs,\n",
    "      signature=\"tokens\",\n",
    "      as_dict=True)\n",
    "\n",
    "    # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
    "    # Use \"sequence_outputs\" for token-level output.\n",
    "    output_layer = bert_outputs[\"pooled_output\"]\n",
    "\n",
    "    hidden_size = output_layer.shape[-1].value\n",
    "    \n",
    "    # Create our own layer to tune for politeness data.\n",
    "    output_weights = tf.get_variable(\n",
    "      \"output_weights\", [num_labels, hidden_size],\n",
    "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "    output_bias = tf.get_variable(\n",
    "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
    "\n",
    "    with tf.variable_scope(\"loss\"):\n",
    "\n",
    "    # Dropout helps prevent overfitting\n",
    "        output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
    "\n",
    "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "    logits = tf.nn.bias_add(logits, output_bias)\n",
    "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "    # Convert labels into one-hot encoding\n",
    "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "\n",
    "    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
    "    # If we're predicting, we want predicted labels and the probabiltiies.\n",
    "    if is_predicting:\n",
    "        return (predicted_labels, log_probs)\n",
    "\n",
    "    # If we're train/eval, compute loss between predicted and actual label\n",
    "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "    loss = tf.reduce_mean(per_example_loss)\n",
    "    return (loss, predicted_labels, log_probs)\n",
    "\n",
    "# model_fn_builder actually creates our model function\n",
    "# using the passed parameters for num_labels, learning_rate, etc.\n",
    "def model_fn_builder(num_labels, learning_rate, num_train_steps,\n",
    "                     num_warmup_steps):\n",
    " #   \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "    def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    " #       \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "        input_ids = features[\"input_ids\"]\n",
    "        input_mask = features[\"input_mask\"]\n",
    "        segment_ids = features[\"segment_ids\"]\n",
    "        label_ids = features[\"label_ids\"]\n",
    "\n",
    "        is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
    "\n",
    "        # TRAIN and EVAL\n",
    "        if not is_predicting:\n",
    "            (loss, predicted_labels, log_probs) = create_model(\n",
    "                is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "\n",
    "            train_op = bert.optimization.create_optimizer(\n",
    "              loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
    "\n",
    "          # Calculate evaluation metrics. \n",
    "            def metric_fn(label_ids, predicted_labels):\n",
    "                accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
    "                return {\"eval_accuracy\": accuracy}\n",
    "\n",
    "            eval_metrics = metric_fn(label_ids, predicted_labels)\n",
    "\n",
    "            if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "                return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                loss=loss,\n",
    "                train_op=train_op)\n",
    "            else:\n",
    "                return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metrics)\n",
    "        else:\n",
    "            (predicted_labels, log_probs) = create_model(is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "\n",
    "            predictions = {'probabilities': log_probs, 'labels': predicted_labels}\n",
    "            return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "        # Return the actual model function in the closure\n",
    "    return model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\PRINGL~1\\AppData\\Local\\Temp\\tmp13v6zui_\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\PRINGL~1\\AppData\\Local\\Temp\\tmp13v6zui_\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\PRINGL~1\\\\AppData\\\\Local\\\\Temp\\\\tmp13v6zui_', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000163EE27C288>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\PRINGL~1\\\\AppData\\\\Local\\\\Temp\\\\tmp13v6zui_', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000163EE27C288>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Training!\n",
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-a662caa84b6a>:33: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-a662caa84b6a>:33: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\bert\\optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\bert\\optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\bert\\optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\bert\\optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\learning_rate_schedule.py:409: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\learning_rate_schedule.py:409: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\bert\\optimization.py:70: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\bert\\optimization.py:70: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\bert\\optimization.py:117: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\bert\\optimization.py:117: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 0 into C:\\Users\\PRINGL~1\\AppData\\Local\\Temp\\tmp13v6zui_\\model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 0 into C:\\Users\\PRINGL~1\\AppData\\Local\\Temp\\tmp13v6zui_\\model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 1.1207838, step = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 1.1207838, step = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 1 vs previous value: 1. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 1 vs previous value: 1. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 31 into C:\\Users\\PRINGL~1\\AppData\\Local\\Temp\\tmp13v6zui_\\model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 31 into C:\\Users\\PRINGL~1\\AppData\\Local\\Temp\\tmp13v6zui_\\model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 63 into C:\\Users\\PRINGL~1\\AppData\\Local\\Temp\\tmp13v6zui_\\model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 63 into C:\\Users\\PRINGL~1\\AppData\\Local\\Temp\\tmp13v6zui_\\model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 94 into C:\\Users\\PRINGL~1\\AppData\\Local\\Temp\\tmp13v6zui_\\model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 94 into C:\\Users\\PRINGL~1\\AppData\\Local\\Temp\\tmp13v6zui_\\model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 93 vs previous value: 93. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 93 vs previous value: 93. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0514793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0514793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 1.1185273, step = 101 (1942.531 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 1.1185273, step = 101 (1942.531 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 126 into C:\\Users\\PRINGL~1\\AppData\\Local\\Temp\\tmp13v6zui_\\model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 126 into C:\\Users\\PRINGL~1\\AppData\\Local\\Temp\\tmp13v6zui_\\model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 157 into C:\\Users\\PRINGL~1\\AppData\\Local\\Temp\\tmp13v6zui_\\model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 157 into C:\\Users\\PRINGL~1\\AppData\\Local\\Temp\\tmp13v6zui_\\model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 189 into C:\\Users\\PRINGL~1\\AppData\\Local\\Temp\\tmp13v6zui_\\model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 189 into C:\\Users\\PRINGL~1\\AppData\\Local\\Temp\\tmp13v6zui_\\model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0517011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0517011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 1.1352651, step = 201 (1934.196 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 1.1352651, step = 201 (1934.196 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 221 into C:\\Users\\PRINGL~1\\AppData\\Local\\Temp\\tmp13v6zui_\\model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 221 into C:\\Users\\PRINGL~1\\AppData\\Local\\Temp\\tmp13v6zui_\\model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 253 into C:\\Users\\PRINGL~1\\AppData\\Local\\Temp\\tmp13v6zui_\\model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 253 into C:\\Users\\PRINGL~1\\AppData\\Local\\Temp\\tmp13v6zui_\\model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 285 into C:\\Users\\PRINGL~1\\AppData\\Local\\Temp\\tmp13v6zui_\\model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 285 into C:\\Users\\PRINGL~1\\AppData\\Local\\Temp\\tmp13v6zui_\\model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.051909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.051909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.9423076, step = 301 (1926.447 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.9423076, step = 301 (1926.447 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 317 into C:\\Users\\PRINGL~1\\AppData\\Local\\Temp\\tmp13v6zui_\\model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 317 into C:\\Users\\PRINGL~1\\AppData\\Local\\Temp\\tmp13v6zui_\\model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 349 into C:\\Users\\PRINGL~1\\AppData\\Local\\Temp\\tmp13v6zui_\\model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 349 into C:\\Users\\PRINGL~1\\AppData\\Local\\Temp\\tmp13v6zui_\\model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 381 into C:\\Users\\PRINGL~1\\AppData\\Local\\Temp\\tmp13v6zui_\\model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 381 into C:\\Users\\PRINGL~1\\AppData\\Local\\Temp\\tmp13v6zui_\\model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.051887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.051887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 1.0311861, step = 401 (1927.266 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 1.0311861, step = 401 (1927.266 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 413 into C:\\Users\\PRINGL~1\\AppData\\Local\\Temp\\tmp13v6zui_\\model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 413 into C:\\Users\\PRINGL~1\\AppData\\Local\\Temp\\tmp13v6zui_\\model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 445 into C:\\Users\\PRINGL~1\\AppData\\Local\\Temp\\tmp13v6zui_\\model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 445 into C:\\Users\\PRINGL~1\\AppData\\Local\\Temp\\tmp13v6zui_\\model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 477 into C:\\Users\\PRINGL~1\\AppData\\Local\\Temp\\tmp13v6zui_\\model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 477 into C:\\Users\\PRINGL~1\\AppData\\Local\\Temp\\tmp13v6zui_\\model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0518763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0518763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 1.0224638, step = 501 (1927.662 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 1.0224638, step = 501 (1927.662 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 509 into C:\\Users\\PRINGL~1\\AppData\\Local\\Temp\\tmp13v6zui_\\model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 509 into C:\\Users\\PRINGL~1\\AppData\\Local\\Temp\\tmp13v6zui_\\model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 541 into C:\\Users\\PRINGL~1\\AppData\\Local\\Temp\\tmp13v6zui_\\model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 541 into C:\\Users\\PRINGL~1\\AppData\\Local\\Temp\\tmp13v6zui_\\model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 572 into C:\\Users\\PRINGL~1\\AppData\\Local\\Temp\\tmp13v6zui_\\model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 572 into C:\\Users\\PRINGL~1\\AppData\\Local\\Temp\\tmp13v6zui_\\model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0517976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0517976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 1.0024273, step = 601 (1930.592 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 1.0024273, step = 601 (1930.592 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 604 into C:\\Users\\PRINGL~1\\AppData\\Local\\Temp\\tmp13v6zui_\\model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 604 into C:\\Users\\PRINGL~1\\AppData\\Local\\Temp\\tmp13v6zui_\\model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 629 into C:\\Users\\PRINGL~1\\AppData\\Local\\Temp\\tmp13v6zui_\\model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 629 into C:\\Users\\PRINGL~1\\AppData\\Local\\Temp\\tmp13v6zui_\\model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loss for final step: 0.87517744.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loss for final step: 0.87517744.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took time  3:23:21.861665\n"
     ]
    }
   ],
   "source": [
    "# Compute train and warmup steps from batch size\n",
    "# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 2.0\n",
    "# Warmup is a period of time where the learning rate \n",
    "# is small and gradually increases--usually helps training.\n",
    "WARMUP_PROPORTION = 0.1\n",
    "# Model configs\n",
    "SAVE_CHECKPOINTS_STEPS = 500\n",
    "SAVE_SUMMARY_STEPS = 100\n",
    "\n",
    "# Compute # train and warmup steps from batch size\n",
    "num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
    "\n",
    "# Specify outpit directory and number of checkpoint steps to save\n",
    "run_config = tf.estimator.RunConfig()\n",
    "\n",
    "model_fn = model_fn_builder(\n",
    "  num_labels=len(label_list),\n",
    "  learning_rate=LEARNING_RATE,\n",
    "  num_train_steps=num_train_steps,\n",
    "  num_warmup_steps=num_warmup_steps)\n",
    "\n",
    "estimator = tf.estimator.Estimator(\n",
    "  model_fn=model_fn,\n",
    "  config=run_config,\n",
    "  params={\"batch_size\": BATCH_SIZE})\n",
    "\n",
    "# Create an input function for training. drop_remainder = True for using TPUs.\n",
    "train_input_fn = bert.run_classifier.input_fn_builder(\n",
    "    features=train_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=True,\n",
    "    drop_remainder=False)\n",
    "\n",
    "print(f'Beginning Training!')\n",
    "current_time = datetime.now()\n",
    "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "print(\"Training took time \", datetime.now() - current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2020-04-12T14:02:15Z\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2020-04-12T14:02:15Z\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\PRINGL~1\\AppData\\Local\\Temp\\tmp13v6zui_\\model.ckpt-629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\PRINGL~1\\AppData\\Local\\Temp\\tmp13v6zui_\\model.ckpt-629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished evaluation at 2020-04-12-14:06:26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished evaluation at 2020-04-12-14:06:26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving dict for global step 629: eval_accuracy = 0.4238683, global_step = 629, loss = 1.0762787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving dict for global step 629: eval_accuracy = 0.4238683, global_step = 629, loss = 1.0762787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 629: C:\\Users\\PRINGL~1\\AppData\\Local\\Temp\\tmp13v6zui_\\model.ckpt-629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 629: C:\\Users\\PRINGL~1\\AppData\\Local\\Temp\\tmp13v6zui_\\model.ckpt-629\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_accuracy': 0.4238683, 'loss': 1.0762787, 'global_step': 629}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input_fn = run_classifier.input_fn_builder(\n",
    "    features=test_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=False,\n",
    "    drop_remainder=False)\n",
    "\n",
    "estimator.evaluate(input_fn=test_input_fn, steps=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = estimator.predict(input_fn=test_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\PRINGL~1\\AppData\\Local\\Temp\\tmp13v6zui_\\model.ckpt-629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\PRINGL~1\\AppData\\Local\\Temp\\tmp13v6zui_\\model.ckpt-629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for i in x:\n",
    "    print(i['labels'])\n",
    "    predictions.append(i['labels'])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 0, 0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 0, 1, 2, 1, 1, 2, 1, 1, 2, 2, 0, 0, 0, 1, 2, 0, 2, 0, 2, 1, 1, 1, 2, 2, 2, 1, 0, 1, 2, 1, 2, 0, 2, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 0, 0, 0, 2, 0, 1, 1, 2, 1, 1, 1, 1, 0, 0, 1, 2, 1, 1, 2, 2, 1, 1, 1, 2, 2, 0, 2, 1, 2, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 2, 0, 0, 1, 2, 2, 0, 2, 1, 2, 0, 2, 2, 1, 1, 1, 2, 0, 1, 1, 1, 2, 2, 1, 0, 1, 1, 2, 0, 0, 2, 1, 2, 2, 1, 1, 0, 1, 1, 1, 1, 1, 2, 1, 2, 0, 0, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 0, 1, 0, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 1, 2, 1, 1, 2, 2, 1, 0, 0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 0, 2, 1, 1, 0, 1, 1, 0, 1, 0, 1, 2, 1, 1, 1, 0, 1, 2, 2, 2, 1, 1, 1, 0, 1, 0, 1, 1, 2, 1, 1, 1, 0, 2, 2, 0, 2, 1, 1, 2, 1, 1, 2, 2, 0, 2, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 0, 1, 2, 1, 1, 0, 0, 2, 2, 1, 1, 0, 0, 1, 1, 1, 1, 2, 0, 1, 0, 2, 0, 1, 0, 1, 0, 2, 1, 0, 2, 2, 1, 1, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 2, 1, 2, 1, 0, 2, 2, 1, 1, 2, 2, 2, 0, 1, 0, 1, 2, 0, 0, 2, 1, 2, 1, 1, 2, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 2, 1, 1, 0, 1, 1, 2, 1, 1, 1, 1, 1, 0, 2, 1, 2, 0, 1, 1, 0, 2, 1, 2, 1, 0, 1, 2, 2, 1, 2, 1, 0, 1, 1, 1, 2, 2, 1, 0, 1, 1, 2, 1, 1, 2, 1, 0, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 2, 2, 2, 1, 2, 2, 1, 0, 2, 2, 0, 1, 1, 0, 2, 2, 1, 2, 0, 1, 1, 0, 1, 1, 0, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 0, 0, 1, 1, 2, 2, 1, 2, 2, 0, 0, 1, 0, 1, 1, 2, 1, 1, 0, 0, 0, 0, 1, 0, 2, 2, 2, 1, 1, 0, 1, 1, 2, 2, 0, 2, 1, 1, 0, 0, 2, 0, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 0, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 0, 0, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 2, 0, 2, 1, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 0, 1, 0, 0, 1, 2, 1, 1, 1, 1, 2, 1, 0, 1, 1, 1, 2, 1, 1, 0, 2, 1, 0, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 2, 1, 0, 0, 1, 0, 2, 1, 1, 0, 1, 2, 2, 1, 0, 2, 2, 1, 1, 2, 1, 1, 0, 0, 1, 0, 1, 2, 0, 1, 1, 2, 1, 1, 0, 1, 1, 2, 1, 0, 0, 0, 0, 0, 2, 0, 1, 1, 1, 2, 1, 1, 0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 1, 0, 2, 0, 0, 0, 1, 0, 1, 2, 0, 1, 0, 2, 0, 2, 1, 2, 0, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 2, 2, 2, 0, 1, 0, 1, 2, 1, 1, 0, 1, 0, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 2, 1, 0, 0, 2, 2, 2, 1, 0, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 0, 1, 2, 1, 1, 1, 2, 1, 1, 2, 0, 2, 1, 2, 2, 0, 2, 1, 0, 0, 0, 0, 1, 1, 0, 1, 2, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 0, 2, 0, 2, 1, 2, 1, 2, 0, 2, 1, 1, 1, 1, 2, 2, 0, 0, 2, 1, 2, 1, 1, 2, 0, 2, 2, 0, 0, 1, 2, 0, 1, 1, 1, 1, 1, 0, 1, 1, 2, 1, 0, 0, 2, 2, 1, 0, 1, 2, 2, 1, 2, 1, 2, 0, 2, 2, 2, 0, 0, 1, 0, 1, 1, 0, 2, 2, 0, 1, 1, 0, 1, 1, 0, 1, 2, 2, 0, 2, 2, 1, 2, 2, 0, 2, 0, 1, 1, 1, 2, 1, 2, 2, 0, 0, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 2, 2, 1, 1, 2, 1, 0, 0, 2, 2, 0, 2, 1, 2, 1, 1, 0, 1, 2, 0, 0, 2, 1, 1, 1, 1, 2, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 2, 1, 0, 0, 1, 2, 2, 1, 1, 0, 2, 1, 1, 1, 1, 0, 1, 2, 2, 0, 1, 2, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 2, 0, 2, 1, 2, 1, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Negative</th>\n",
       "      <td>261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neutral</th>\n",
       "      <td>355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive</th>\n",
       "      <td>356</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          doc\n",
       "target       \n",
       "Negative  261\n",
       "Neutral   355\n",
       "Positive  356"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.groupby('target').count()\n",
    "#So best we can get by randomly guessing positive or neutral all the time is 36%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.replace(to_replace=[0, 1, 2],\n",
    "           value= [2, 0, 1], \n",
    "           inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 61 228  66]\n",
      " [ 93 162 101]\n",
      " [ 63  99  99]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Print the confusion matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "plt.colorbar()\n",
    "#tick_marks = np.arange(len(test.[target))\n",
    "#plt.xticks(tick_marks, test.target, rotation=45)\n",
    "#plt.yticks(tick_marks, test.target)\n",
    "plt.xlabel(\"Predicted Outcome\")\n",
    "plt.ylabel(\"True Outcome\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.24      0.26       261\n",
      "           1       0.47      0.64      0.54       355\n",
      "           2       0.38      0.28      0.32       356\n",
      "\n",
      "    accuracy                           0.40       972\n",
      "   macro avg       0.38      0.39      0.38       972\n",
      "weighted avg       0.39      0.40      0.39       972\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn. metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
